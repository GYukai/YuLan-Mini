import os
import json
import glob
import pathlib
import numpy as np
import sys
import threading
from tqdm import tqdm
import multiprocessing as mp
import pyarrow
from pyarrow import parquet as pq

# split data by 0.01B tokens (this is a soft limit)
MAX_TOKEN = int(0.01 * 1000 * 1000 * 1000)

father_datasets = list(sys.argv)[1:]

# replace raw data path with input_ids path
raw_data_prefix = os.environ["RAW_DATA_PREFIX"]
input_ids_prefix = os.environ["INPUT_IDS_PREFIX"]

father_datasets = [
    i.replace(raw_data_prefix, input_ids_prefix)
    for i in father_datasets
]


def warn(msg):
    print("\033[0;33m" + msg + "\033[0m")


def process_file(src_folder, src_file, print_id, write_format="parquet", is_last=False, last_states=None, max_part=-1):

    def write_to_file(all_data, num_tokens: int, cur_idx: int, tokens_num: list):
        """Write splitted data and metadata."""

        cur_idx = f"{cur_idx:04d}"
        if write_format == "jsonl":
            tgt_path = os.path.join(src_folder, "splitted_part-{}.jsonl".format(cur_idx))
            print(print_id, "updating", tgt_path, num_tokens)
            with open(tgt_path, "w") as fout:
                for tmp_data in all_data:
                    fout.write(json.dumps({"input_ids": tmp_data}, ensure_ascii=False) + "\n")

        elif write_format == "parquet":
            tgt_path = os.path.join(src_folder, "splitted_part-{}.parquet".format(cur_idx))
            print(print_id, "updating", tgt_path, num_tokens)
            arr = pyarrow.array(all_data)
            pq.write_table(pyarrow.Table.from_arrays([arr], ["input_ids"]), tgt_path)

        tokens_num_tgt_path = os.path.join(src_folder, "splitted_part-{}-metadata.json".format(cur_idx))
        with open(tokens_num_tgt_path, "w") as fout:
            json.dump({"total_tokens_num": num_tokens, "tokens_num": tokens_num}, fout, indent=2)

    def load_data_jsonl(fin):
        """Read one line and return as parsed json data."""
        data = fin.readline()
        if not data:
            return None
        else:
            json_data = json.loads(data)
            new_data = json_data["input_ids"]
            return (new_data, len(json_data["input_ids"]))

    all_data = []
    tokens_num = []
    num_tokens = 0
    cur_idx = max_part + 1
    if last_states is not None:
        all_data, num_tokens, cur_idx, tokens_num = last_states

    if src_file.endswith(".parquet"):

        # parquet read
        table = pq.read_table(src_file)
        all_all_data = table["input_ids"].to_pylist()
        for ids in all_all_data:
            all_data.append(ids)
            tokens_num.append(len(ids))
            num_tokens += len(ids)
            if num_tokens > MAX_TOKEN:
                # flush new splitted data to file
                write_to_file(all_data, num_tokens, cur_idx, tokens_num)
                all_data = []
                tokens_num = []
                num_tokens = 0
                cur_idx += 1
                print(print_id, "next split", cur_idx)

        # trailing lines
        if len(all_data) > 0 and is_last:
            write_to_file(all_data, num_tokens, cur_idx, tokens_num)

    elif src_file.endswith(".jsonl"):

        # jsonl read line by line
        with open(src_file) as fin:
            while True:
                data = load_data_jsonl(fin)
                if data is None:
                    break

                # add data
                all_data.append(data[0])
                tokens_num.append(data[1])
                num_tokens += data[1]
                if num_tokens > MAX_TOKEN:
                    # flush new splitted data to file
                    write_to_file(all_data, num_tokens, cur_idx, tokens_num)
                    all_data = []
                    tokens_num = []
                    num_tokens = 0
                    cur_idx += 1

        # trailing lines of whole wo_ppl folder
        if len(all_data) > 0 and is_last:
            write_to_file(all_data, num_tokens, cur_idx, tokens_num)

    with open("datasets_to_delete.txt", "a") as f:
        f.write(src_file + "\n")
        print(print_id, src_file, "added to delete list")
    return all_data, num_tokens, cur_idx, tokens_num


def do_parts(src_folder, src_files, max_part: int):
    """Process all parts (each part is a tokenized dataset generated by ONE thread in `tokenize_text.py`) in one folder."""

    last_states = None
    sort_files = sorted(src_files, key=lambda x: int(x.split("-")[-1].split(".")[0]))
    length = len(sort_files)
    for idx, src_file in enumerate(sort_files):
        last_states = process_file(src_folder, src_file, last_states=last_states, is_last=(idx == length - 1), max_part=max_part, print_id=os.getpid())


def process_dataset(fd):
    datasets = os.listdir(fd)
    folder2file = {}
    for dataset_name in tqdm(datasets):
        raw_src_folder = os.path.join(fd, dataset_name)
        print("Processing {} ...".format(raw_src_folder))

        try:
            for root_dir, _, files in os.walk(raw_src_folder, topdown=False):
                max_part = max([int(fp.split("-")[-1].split(".")[0]) for fp in files if "splitted_part" in fp and "tokens" not in fp], default=-1)
                for fp in files:
                    if "sort" in fp or "splitted_part" in fp:
                        continue
                    if not fp.endswith(".jsonl") and not fp.endswith(".parquet"):
                        continue
                    if root_dir not in folder2file:
                        folder2file[root_dir] = ([], max_part)
                    folder2file[root_dir][0].append(os.path.join(root_dir, fp))

        except FileNotFoundError as e:
            print("Error Dataset: {} ({})".format(dataset_name, e))
            continue
        except NotADirectoryError as e:
            print("Error Dataset: {} ({})".format(dataset_name, e))
            continue

        if len(folder2file) == 0:
            print("Error Dataset: {} (len(folder2file) == 0)".format(dataset_name))
            continue

    # process all files in parallel
    folder_n = len(folder2file)
    p = mp.Pool(32)
    for idx, (src_folder, (src_files, max_part)) in enumerate(folder2file.items()):
        print(f"submitting {idx + 1} / {folder_n}", src_folder, len(src_files))
        p.apply_async(do_parts, args=(src_folder, src_files, max_part))
    p.close()
    p.join()

    warn(f"finished {raw_src_folder}")


if __name__ == "__main__":
    try:
        for fd in father_datasets:
            process_dataset(fd)
    except (Exception, KeyboardInterrupt) as e:
        warn("Early abortion. Please delete manully files in datasets_to_delete.txt")
        raise e
